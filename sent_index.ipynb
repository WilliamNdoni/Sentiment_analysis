{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3d5a05",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS :GOOGLE AND APPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33122cf",
   "metadata": {},
   "source": [
    "## Business understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38180e",
   "metadata": {},
   "source": [
    "Google and Apple are multinational techonology companies well known for their products such as google sheets (from Google) and iPhone (from Apple).The companies have come up with ways to get their customer feedback such as,in app user feedback and rating, getting the tweets from Twitter(now X),among many others. However,since the companies are multinational it can be really difficult and tiresome to read through the millions of feedback or tweets from multiple apps in order to get the customers' view or sentiment about a product.As a result,the companies want to build a model that can rate the sentiment of a tweet or text based on its content.This will enable the companies to make improvements on their products or services to improve customer satisfaction and even attract more customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a3d34",
   "metadata": {},
   "source": [
    "## Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56cfa8",
   "metadata": {},
   "source": [
    "The data used in this project was extracted from [data.world](https://data.world/crowdflower/brands-and-product-emotions). It contains tweets related to Google and Apple products which were ranked as negative,positive or neutral. This dataset contains over 9000 tweets.In addition to the tweets and their ratings,the dataset contains a column that shows the product the tweet is directed to.This dataset will be of great help when building a model for our sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefdf54",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857263b",
   "metadata": {},
   "source": [
    "First,we will prepare the data with nltk and develop a model from the resulting data.This will act as our basic model.From there we will build models with RNN using LSTMs and GRUs and pick the best performing model.The data preparation for RNN models is different and will not incoporate nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82f946",
   "metadata": {},
   "source": [
    "#### Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c8a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet',quiet=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3a0213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset using pandas\n",
    "raw_df = pd.read_csv('Data/judge-1377884607_tweet_product_company.csv',encoding='ISO-8859-1')\n",
    "# Taking a look at the dataset\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2c968",
   "metadata": {},
   "source": [
    "The dataset contains columns with really long names.We can start by renaming the columns to have shorter column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004155b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Renaming the columns\n",
    "raw_df.columns =['tweet','product','emotion']\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019e735",
   "metadata": {},
   "source": [
    "#### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bae9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking a look at the emotion column\n",
    "raw_df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e864107",
   "metadata": {},
   "source": [
    "We want to develop a model that can tell whether a tweet is positive,negative or neutral.The emotion column consist of four categories.The `I can't tell` category will be dropped since it is of no use to our model. One may consider changing this category into the no emotion category but it might ruin the model in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a94370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the 'I can't tell' category from the emotion column\n",
    "df_3cat = raw_df[raw_df['emotion'] != \"I can't tell\"]\n",
    "# Checking the remaining categories in the emotion column\n",
    "df_3cat.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1cd5978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at one of the tweets\n",
    "df_3cat.tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfe806",
   "metadata": {},
   "source": [
    "Since these are tweets from Twitter (now X) they have hashtags and username tags(@) which have no value in determining the sentiment of a tweet or text.These tags should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b73475",
   "metadata": {},
   "source": [
    "#### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a08656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet         1\n",
       "product    5655\n",
       "emotion       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "df_3cat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f6ef2",
   "metadata": {},
   "source": [
    "The `tweet` column has only one missing value while the `product` column has more than half of the observations as missing.The `product` column can be dropped since we only need the other two columns to build a model for sentiment analysis.The row containing the missing tweet will be dropped too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9035b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the product column\n",
    "df_2col = df_3cat.drop('product',axis=1)\n",
    "# Dropping the row containing the missing tweet\n",
    "df_2col.dropna(inplace=True)\n",
    "# Checking for missing values\n",
    "df_2col.isna().sum()\n",
    "# # Reseting the index of the dataframe\n",
    "df_2col.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa769c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a RegexpTokenizer that will include words with apostrophes\n",
    "tokenizer = RegexpTokenizer(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "# Creating a list of stopwords to exlude numbers and the sxsw tag\n",
    "stopwords_list = stopwords.words('english')+['sxsw']+['0','1','2','3','4','5','6','7','8','9']\n",
    "# Creating an instance of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9fb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function that will produce the appropriate tokens\n",
    "def word_preprocessor(text,tokenizer,stopwords_list,lemmatizer):\n",
    "#     removing capital letters in the text\n",
    "    low = text.lower()\n",
    "#     tokenizing the text\n",
    "    tokens = tokenizer.tokenize(low)\n",
    "#     removing stopwords from the tokens\n",
    "    no_stopwords_list = [word for word in tokens if word not in stopwords_list]\n",
    "#     performing lemmatization\n",
    "#     we can remove the first word from the tweets since it is a name tag\n",
    "    preprocessed_text = [lemmatizer.lemmatize(word) for word in no_stopwords_list[1:]]\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e678df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3g',\n",
       " 'iphone',\n",
       " 'hr',\n",
       " 'tweeting',\n",
       " 'rise_austin',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'station']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if the function works\n",
    "word_preprocessor( df_2col.tweet[0],tokenizer,stopwords_list,lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8180e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the function to the dataset and creating a new column with the preprocessed tweets\n",
    "df_2col['preprocessed'] = df_2col.tweet.apply\\\n",
    "(lambda x: word_preprocessor(x,tokenizer,stopwords_list,lemmatizer) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a15cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[3g, iphone, hr, tweeting, rise_austin, dead, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[know, fludapp, awesome, ipad, iphone, app, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[wait, ipad, also, sale]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[year's, festival, crashy, year's, iphone, app]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, google, ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet           emotion  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  Negative emotion   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  Positive emotion   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  Positive emotion   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  Negative emotion   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Positive emotion   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [3g, iphone, hr, tweeting, rise_austin, dead, ...  \n",
       "1  [know, fludapp, awesome, ipad, iphone, app, li...  \n",
       "2                           [wait, ipad, also, sale]  \n",
       "3    [year's, festival, crashy, year's, iphone, app]  \n",
       "4  [great, stuff, fri, marissa, mayer, google, ti...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the new dataframe\n",
    "df_2col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecd7d8",
   "metadata": {},
   "source": [
    "Next we will create a rank column to contain integers as follows:\n",
    "- Positive emotion = 1\n",
    "- Negative emotion = -1\n",
    "- No emotion toward brand or product = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192d97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function to encode the categories to integers\n",
    "def encoder(text):\n",
    "    if text == 'Positive emotion':\n",
    "        return 1\n",
    "    if text == 'Negative emotion':\n",
    "        return 2 # since to_categorical is designed to work with non_negative integers\n",
    "    else :\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c1d436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[3g, iphone, hr, tweeting, rise_austin, dead, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[know, fludapp, awesome, ipad, iphone, app, li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[wait, ipad, also, sale]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[year's, festival, crashy, year's, iphone, app]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, google, ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet           emotion  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  Negative emotion   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  Positive emotion   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  Positive emotion   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  Negative emotion   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Positive emotion   \n",
       "\n",
       "                                        preprocessed  rank  \n",
       "0  [3g, iphone, hr, tweeting, rise_austin, dead, ...     2  \n",
       "1  [know, fludapp, awesome, ipad, iphone, app, li...     1  \n",
       "2                           [wait, ipad, also, sale]     1  \n",
       "3    [year's, festival, crashy, year's, iphone, app]     2  \n",
       "4  [great, stuff, fri, marissa, mayer, google, ti...     1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new column with the rankings\n",
    "df_2col['rank'] = df_2col.emotion.apply( lambda x: encoder(x) )\n",
    "df_2col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87754273",
   "metadata": {},
   "source": [
    "Next we are going to join the preprocessed column to contain single strings per row to make them compatible to sklearn's CountVectorizer and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a07bb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>rank</th>\n",
       "      <th>joined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[3g, iphone, hr, tweeting, rise_austin, dead, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3g iphone hr tweeting rise_austin dead need up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[know, fludapp, awesome, ipad, iphone, app, li...</td>\n",
       "      <td>1</td>\n",
       "      <td>know fludapp awesome ipad iphone app likely ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[wait, ipad, also, sale]</td>\n",
       "      <td>1</td>\n",
       "      <td>wait ipad also sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[year's, festival, crashy, year's, iphone, app]</td>\n",
       "      <td>2</td>\n",
       "      <td>year's festival crashy year's iphone app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, google, ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>great stuff fri marissa mayer google tim o'rei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet           emotion  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  Negative emotion   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  Positive emotion   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  Positive emotion   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  Negative emotion   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Positive emotion   \n",
       "\n",
       "                                        preprocessed  rank  \\\n",
       "0  [3g, iphone, hr, tweeting, rise_austin, dead, ...     2   \n",
       "1  [know, fludapp, awesome, ipad, iphone, app, li...     1   \n",
       "2                           [wait, ipad, also, sale]     1   \n",
       "3    [year's, festival, crashy, year's, iphone, app]     2   \n",
       "4  [great, stuff, fri, marissa, mayer, google, ti...     1   \n",
       "\n",
       "                                         joined_text  \n",
       "0  3g iphone hr tweeting rise_austin dead need up...  \n",
       "1  know fludapp awesome ipad iphone app likely ap...  \n",
       "2                                wait ipad also sale  \n",
       "3           year's festival crashy year's iphone app  \n",
       "4  great stuff fri marissa mayer google tim o'rei...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2col['joined_text'] = df_2col['preprocessed'].str.join(\" \")\n",
    "df_2col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b390e1c",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df91f5",
   "metadata": {},
   "source": [
    "The dataset will be split into train set,validation set and test set.However when developing RNN models we will add a parameter for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f61e245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the inputs and targets\n",
    "X= df_2col['joined_text']\n",
    "y= df_2col['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6e9b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the relevant libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df52e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset with a test_size of 0.2 and random_state of 42\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4a394",
   "metadata": {},
   "source": [
    "### Word vectorization using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc4f59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating CountVectorizer object\n",
    "count_vectorizer =CountVectorizer()\n",
    "# fitting the vectorizer on the train set\n",
    "count_vectorizer.fit(X_train)\n",
    "# transforming the train and test sets\n",
    "X_train_vectorized = count_vectorizer.transform(X_train)\n",
    "X_test_vectorized = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea595b0a",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07deac1",
   "metadata": {},
   "source": [
    "#### Building a baseline model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8953a3",
   "metadata": {},
   "source": [
    " We will build a baseline model using the outputs of the CountVectorizer.Thebaseline model will be a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b735f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=15, min_samples_split=300,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=15, min_samples_split=300,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=300,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the relevant libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# instantiating the DecisionTreeClassifier with certain parameters\n",
    "tree_clf = DecisionTreeClassifier(criterion='entropy',min_samples_split=300,random_state=42,max_depth=15)\n",
    "# fitting the model\n",
    "tree_clf.fit(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "654e0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.92      0.76      1094\n",
      "           1       0.59      0.22      0.32       568\n",
      "           2       0.62      0.08      0.14       126\n",
      "\n",
      "    accuracy                           0.64      1788\n",
      "   macro avg       0.62      0.41      0.41      1788\n",
      "weighted avg       0.63      0.64      0.58      1788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# making predicitions for the test set\n",
    "y_test_pred = tree_clf.predict(X_test_vectorized)\n",
    "# creating a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08351196",
   "metadata": {},
   "source": [
    "Our baseline model has an accuracy of 64%.Next we will try to build models that have a better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5858929",
   "metadata": {},
   "source": [
    "#### Building a second model using tf-idf vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e23fc7",
   "metadata": {},
   "source": [
    "Tf-idf vectorization tends to give higher weights to words that have a low document frequency and a high term frequency.We will perform vectorization using tf-idf to see if it will improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aeea2c",
   "metadata": {},
   "source": [
    "#### Vectorization using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1ebb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating TfidfVectorizer object\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "# fitting the vectorizer on the train set\n",
    "tf_idf_vectorizer.fit(X_train)\n",
    "# transforming the train and test sets\n",
    "X_train_tfvectorized = tf_idf_vectorizer.transform(X_train)\n",
    "X_test_tfvectorized = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b2e5f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=15, min_samples_split=300,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=15, min_samples_split=300,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=300,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a model using the outputs TfidfVectorizer\n",
    "tf_tree_clf = DecisionTreeClassifier(criterion='entropy',\n",
    "                                     min_samples_split=300,random_state=42,max_depth=15)\n",
    "tf_tree_clf.fit(X_train_tfvectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "510af068",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 8725 features, but DecisionTreeClassifier is expecting 7630 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# making predicitions for the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtree_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tfvectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# creating a classification report\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test,y_test_pred))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\tree\\_classes.py:500\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    499\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 500\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    502\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\tree\\_classes.py:460\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    468\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[0;32m    469\u001b[0m ):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8725 features, but DecisionTreeClassifier is expecting 7630 features as input."
     ]
    }
   ],
   "source": [
    "# making predicitions for the test set\n",
    "y_test_pred = tree_clf.predict(X_test_tfvectorized)\n",
    "# creating a classification report\n",
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac7853",
   "metadata": {},
   "source": [
    "With the same parameters this model performs poorer than the basic model.Next we will try to build models using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a208a6",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (with LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3552bc",
   "metadata": {},
   "source": [
    "In order to build RNN models we need to one hot encode the `rank` column using to_categorical which is a utility function found in Keras library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3642c",
   "metadata": {},
   "source": [
    "#### One hot encoding the rank column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4272bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant libraries\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "953da77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset with a test_size of 0.2 and random_state of 42\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_2col['tweet'],y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88a4a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding the rank column in all the sets\n",
    "y_train_labels = to_categorical(y_train)\n",
    "y_test_labels = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "01fe0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant libraries for building RNNs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding,GRU\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e78ff09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset for use by keras\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(X_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a7192eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the sequential model\n",
    "model = Sequential()\n",
    "# Adding an embedding layer\n",
    "embedding_size = 70\n",
    "model.add(Embedding(20000, embedding_size))\n",
    "# Adding an LTSM layer\n",
    "model.add(LSTM(25, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "# Performing dropout regularization\n",
    "model.add(Dropout(0.5))\n",
    "# Adding the outputlayer\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "25a52610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f8da22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "# Instantiating callbacks\n",
    "call_backs = [EarlyStopping(monitor='val_loss', patience=7),\n",
    "             ModelCheckpoint(\"best_model.h5\", monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "273af8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 7s 200ms/step - loss: 1.0387 - accuracy: 0.5719 - val_loss: 0.9507 - val_accuracy: 0.6049\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 3s 166ms/step - loss: 0.9086 - accuracy: 0.5801 - val_loss: 0.8320 - val_accuracy: 0.6049\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 4s 194ms/step - loss: 0.8671 - accuracy: 0.5841 - val_loss: 0.8291 - val_accuracy: 0.6049\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 4s 186ms/step - loss: 0.8556 - accuracy: 0.5986 - val_loss: 0.8130 - val_accuracy: 0.6049\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 3s 174ms/step - loss: 0.8174 - accuracy: 0.6217 - val_loss: 0.7785 - val_accuracy: 0.6266\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 3s 165ms/step - loss: 0.7576 - accuracy: 0.6726 - val_loss: 0.7521 - val_accuracy: 0.6720\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 3s 157ms/step - loss: 0.6929 - accuracy: 0.7246 - val_loss: 0.7523 - val_accuracy: 0.6685\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 3s 159ms/step - loss: 0.6278 - accuracy: 0.7644 - val_loss: 0.7802 - val_accuracy: 0.6720\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 3s 159ms/step - loss: 0.5708 - accuracy: 0.7952 - val_loss: 0.7890 - val_accuracy: 0.6664\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 3s 160ms/step - loss: 0.5158 - accuracy: 0.8199 - val_loss: 0.8041 - val_accuracy: 0.6699\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 3s 168ms/step - loss: 0.4825 - accuracy: 0.8363 - val_loss: 0.8663 - val_accuracy: 0.6657\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 4s 178ms/step - loss: 0.4550 - accuracy: 0.8377 - val_loss: 0.8623 - val_accuracy: 0.6657\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 4s 183ms/step - loss: 0.4109 - accuracy: 0.8513 - val_loss: 0.8958 - val_accuracy: 0.6629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2662a801b10>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y_train_labels, epochs=20, batch_size=300, validation_split=0.2,callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e41ea",
   "metadata": {},
   "source": [
    "From the results above we can tel that the saved model (model with the best metrics) has an accuracy of 67% which is an improvement from our previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf2402",
   "metadata": {},
   "source": [
    "Next we will try to build a model using the embeddings of the Glove Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1f4d9",
   "metadata": {},
   "source": [
    "### Using Glove to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a673a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
